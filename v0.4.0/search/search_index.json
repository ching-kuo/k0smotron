{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"k0smotron - The Kubernetes control plane manager","text":"<p>From pets to cattle: Streamline your Kubernetes control plane management with k0smotron</p>"},{"location":"#features","title":"Features","text":""},{"location":"#kubernetes-in-kubernetes","title":"Kubernetes-in-Kubernetes","text":"<p>k0smotron allows you to easily create and manage the clusters in an existing Kubernetes cluster. This allows unparalled scalability and flexibility when you have to work with many clusters. It allows truly homogenous setup for all control planes and thus eases the maintenance burden.</p>"},{"location":"#true-control-and-worker-plane-separation","title":"True control and worker plane separation","text":"<p>Using k0smotron the clusters controlplane and workerplane are truly separated. The controlplane, running on an existing cluster has no direct networking connection to the workerplane. This is similar patter nhow all the major cloud providers separate the control and worker planes on the managed clusters. </p>"},{"location":"#bring-your-own-workers","title":"Bring your own workers","text":"<p>With k0smotron you can connect worker nodes from ANY infrastructure to your cluster control plane. </p>"},{"location":"#how-does-it-work","title":"How does it work","text":"<p>You install k0smotron operator into an existing Kubernetes cluster. k0smotron operator will create and manage k0s control planes in that cluster. It leverages the natural pattern of working with custom resources to manage the lifecycle of the k0s control planes. k0smotron will automatically create all the needed Kubernetes lower level constructs, such as pods, configmaps etc., to </p> <p>k0smotron is an Kubernetes operator designed to manage the lifecycle of k0s control planes in a Kubernetes (any distro) cluster. By running the control plane on a k8s cluster we can enjoy and leverage the high availability and auto-healing functionalities of the underlying cluster, a.k.a Mothership.</p> <p></p>"},{"location":"#use-cases","title":"Use cases","text":""},{"location":"#cicd","title":"CI/CD","text":"<p>Often when running integration and end-to-end testing for your software running in Kubernetes you need somewhat temporary clusters in CI. Why not leverage the true flexibility and create those clusters on-demand using k0smotron. Creating a controlplane is as easy as creating a custom resource, so is the deletion of it. No more long living snowflake clusters for CI purposes.</p>"},{"location":"#edge","title":"Edge","text":"<p>Running Kubernetes on the network edge usually means running in low resource infrastructure. What this often means is that setting up the controlplane is either a challenge or a mission impossible. Running the controlplane on a existing cluster, on a separate dedicated infrastructure, removes that challenge and let's you focus on the real edge. </p> <p>Running on the edge often also means large number of clusters to manage. Do you really want to dedicate nodes for each cluster controlplane and manage all the infrastructure for those?</p>"},{"location":"#multi-cloud","title":"Multi-cloud","text":"<p>With k0smotron you can run your control plane management cluster (a.k.a Mothership) in one cloud provider and the workloads in various other cloud providers. This allows you to build and maintain a very streamlined approach to multi cloud.</p>"},{"location":"capi-bootstrap/","title":"Cluster API - Bootstrap provider","text":"<p>k0smotron can act as a Cluster API Boostrap provider. As k0smotron itself will run the cluster control plane within the management cluster the bootstrap provider is (currently) focusing only on bootstrapping worker nodes.</p> <p>As with any other Cluster API provider you must create a <code>Machine</code> object with a reference to a bootstrap provider:</p> <pre><code>apiVersion: cluster.x-k8s.io/v1beta1\nkind: Machine\nmetadata:\nname: cp-test-0\nspec:\nclusterName: cp-test\nbootstrap:\nconfigRef: # This triggers our controller to create cloud-init secret\napiVersion: bootstrap.cluster.x-k8s.io/v1beta1\nkind: K0sWorkerConfig\nname: cp-test-0\ninfrastructureRef:\napiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: HCloudMachine\nname: cp-test-0\n</code></pre> <p>Next we need to provide the configuration for the bootstrapping:</p> <pre><code>apiVersion: bootstrap.cluster.x-k8s.io/v1beta1\nkind: K0sWorkerConfig\nmetadata:\nname: cp-test-0\nspec:\n</code></pre> <p>As k0s comes with all the needed bells and whistles to get k8s worker node up, we do not need to specify any details in this simple example.</p> <p>Check the examples pages for more detailed examples how k0smotron can be used with various Cluster API infrastructure providers.</p>"},{"location":"capi-controlplane/","title":"Cluster API - Control Plane provider","text":"<p>k0smotron can act as a control plane provider via usage of <code>K0smotronControlPlane</code> CRDs.</p> <p>As per usual, you need to define a <code>Cluster</code> object given with a reference to control plane provider: <pre><code>apiVersion: cluster.x-k8s.io/v1beta1\nkind: Cluster\nmetadata:\nname: cp-test\nspec:\nclusterNetwork:\npods:\ncidrBlocks:\n- 10.244.0.0/16\nservices:\ncidrBlocks:\n- 10.96.0.0/12\ncontrolPlaneRef:\napiVersion: controlplane.cluster.x-k8s.io/v1beta1\nkind: K0smotronControlPlane\nname: cp-test\n</code></pre></p> <p>Next we need to provider the configuration for the actual <code>K0smotronControlPlane</code>:</p> <pre><code>apiVersion: controlplane.cluster.x-k8s.io/v1beta1\nkind: K0smotronControlPlane\nmetadata:\nname: cp-test\nspec:\nk0sVersion: v1.27.2-k0s.0\npersistence:\ntype: emptyDir\nservice:\ntype: LoadBalancer\n# apiPort: 6443\n# konnectivityPort: 8132\nannotations:\nload-balancer.hetzner.cloud/location: fsn1\n</code></pre> <p>The <code>K0smotronControlPlane.spec</code> field is a direct mapping of the \"standalone\" k0smotron cluster configuration.</p>"},{"location":"capi-examples/","title":"Cluster API - Examples","text":"<p>On these pages you'll find set of examples how to use k0smotron as a Cluster API provider for various cloud providers.</p>"},{"location":"capi-examples/#prerequisites","title":"Prerequisites","text":"<p>All the examples assume following prerequisites.</p>"},{"location":"capi-examples/#management-cluster","title":"Management cluster","text":"<p>You need to have an existing cluster you'll use as the management cluster. Naturally we expect you point your <code>kubectl</code> or any other client tooling you use to use that cluster.</p> <p>If you do not yet have a management cluster in your hands remember that you can create one using k0s super easily.</p>"},{"location":"capi-examples/#k0smotron","title":"k0smotron","text":"<p>To install k0smotron on the management cluster follow the installation guide.</p>"},{"location":"capi-examples/#clusterctl","title":"clusterctl","text":"<p>You need to have <code>clusterctl</code> installed.</p>"},{"location":"capi-hetzner/","title":"Cluster API - Hetzner","text":"<p>In this guide we will show you how to use Hetzner infrastructure for the worker plane while using k0smotron control plane.</p>"},{"location":"capi-hetzner/#preparations","title":"Preparations","text":"<p>To initialize the management cluster with Hetzner infrastrcture provider you can run:</p> <pre><code>clusterctl init --core cluster-api --infrastructure hetzner\n</code></pre> <p>For more details on Hetzner Cluster API provider see it's docs.</p>"},{"location":"capi-hetzner/#token","title":"Token","text":"<p>To be able to provision the infrastructure Hetzner provider will need a token to interact with Hetzner API.  You'll find this if you click on the project and go to \"security\" on Hetzner console.</p>"},{"location":"capi-hetzner/#creating-a-cluster","title":"Creating a cluster","text":"<pre><code>apiVersion: cluster.x-k8s.io/v1beta1\nkind: Cluster\nmetadata:\nname: cp-test\nspec:\nclusterNetwork:\npods:\ncidrBlocks:\n- 10.244.0.0/16\nservices:\ncidrBlocks:\n- 10.96.0.0/12\ncontrolPlaneRef:\napiVersion: controlplane.cluster.x-k8s.io/v1beta1\nkind: K0smotronControlPlane # This tells that k0smotron should create the controlplane\nname: cp-test\ninfrastructureRef:\napiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: HetznerCluster\nname: cp-test\n---\napiVersion: controlplane.cluster.x-k8s.io/v1beta1\nkind: K0smotronControlPlane # This is the config for the controlplane\nmetadata:\nname: cp-test\nspec:\nk0sVersion: v1.27.2-k0s.0\npersistence:\ntype: emptyDir\nservice:\ntype: LoadBalancer\napiPort: 6443\nkonnectivityPort: 8132\nannotations:\nload-balancer.hetzner.cloud/location: fsn1\n---\napiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: HetznerCluster\nmetadata:\nname: cp-test\nspec:\ncontrolPlaneLoadBalancer:\nenabled: false\ncontrolPlaneEndpoint: # This is just a placeholder, can be anything as k0smotron will overwrite it\nhost: \"1.2.3.4\"\nport: 6443\ncontrolPlaneRegions:\n- fsn1\nhetznerSecretRef:\nname: hetzner\nkey:\nhcloudToken: hcloud\n---\napiVersion: cluster.x-k8s.io/v1beta1\nkind: Machine\nmetadata:\nname: cp-test-0\nspec:\nclusterName: cp-test\nbootstrap:\nconfigRef: # This triggers our controller to create cloud-init secret\napiVersion: bootstrap.cluster.x-k8s.io/v1beta1\nkind: K0sWorkerConfig\nname: cp-test-0\ninfrastructureRef:\napiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: HCloudMachine\nname: cp-test-0\n\n---\napiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: HCloudMachine\nmetadata:\nname: cp-test-0\nspec:\nimageName: ubuntu-22.04\ntype: cx21\nsshKeys:\n- name: your-ssh-key-name\n---\napiVersion: bootstrap.cluster.x-k8s.io/v1beta1\nkind: K0sWorkerConfig\nmetadata:\nname: cp-test-0\nspec:\n---\napiVersion: v1\nkind: Secret\ndata:\nhcloud: &lt;base64 encoded token&gt;\nmetadata:\nname: hetzner\n</code></pre> <p>In the case of <code>HetznerCluster.spec.controlPlaneEndpoint</code> you can add any valid address. k0smotron will overwrite these are automatically once it gets the control plane up-and-running. You do need to specify some placeholder address as the <code>HetznerCluster</code> object has those marked as mandatory fields.</p> <p>Once you apply the manifests to the management cluster it'll take couple of minutes to provision everything. In the end you should see something like this:</p> <pre><code>% kubectl get cluster,machine\nNAME                               PHASE         AGE     VERSION\ncluster.cluster.x-k8s.io/cp-test   Provisioned   3m51s   \n\nNAME                                 CLUSTER   NODENAME   PROVIDERID          PHASE         AGE     VERSION\nmachine.cluster.x-k8s.io/cp-test-0   cp-test              hcloud://12345678   Provisioned   3m50s\n</code></pre>"},{"location":"capi-hetzner/#accessing-the-workload-cluster","title":"Accessing the workload cluster","text":"<p>To access the workload (a.k.a child) cluster we can get the kubeconfig for it with <code>clusterctl get kubeconfig cp-test</code>. You can then save it to disk and/or import to your favorite tooling like Lens</p>"},{"location":"cluster-api/","title":"k0smotron as Cluster API provider","text":"<p>k0smotron can act as a Cluster API provider for both control planes and for <code>Machine</code> bootstrapping.</p> <p>Note: Cluster API providers in k0smotron are very experimental still. We're iterating fast on these but we'd definitely value your feedback how it behaves with various infrastructure providers.</p>"},{"location":"cluster-api/#control-plane-provider","title":"Control Plane provider","text":"<p>When k0smotron acts as a control plane provider it will create and manage the cluster control plane within the management cluster, just as in the standalone case.</p>"},{"location":"cluster-api/#bootstrap-provider","title":"Bootstrap provider","text":"<p>k0smotron can also act as a bootstrap provider for worker nodes you want to manage via Cluster API. When k0smotron detects a new node that needs to be added to the cluster it will automatically create a new join token needed for the node and creates the provisioning cloud-init script for the node. Once Cluster API controllers sees the node initialization script in place (in a secret) the infrastructure provider will create the needed resources (usually VMs in cloud provider infrastructure) with the k0smotron created cloud-init script.</p>"},{"location":"cluster/","title":"Creating a cluster","text":"<p>The following example creates a simple cluster named <code>k0smotron-test</code>:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: k0smotron.io/v1beta1\nkind: Cluster\nmetadata:\n  name: k0smotron-test\nspec: null\nEOF\n</code></pre> <p>This triggers k0smotron controllers to setup the control plane in pods. Once k0smotron is done you can get the admin access kubeconfig:</p> <pre><code>kubectl get secret k0smotron-test-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; ~/.kube/child.conf\n</code></pre> <p>Warning: Depending on your configuration, the admin kubeconfig may not be pointing to the right address. If the kubeconfig doesn't work by default, you'll need to set the right value in <code>&lt;server URL&gt;</code>.</p> <pre><code>apiVersion: v1\nclusters:\n- cluster:\nserver: &lt;server URL&gt;\ncertificate-authority-data: &lt;redacted&gt;\nname: k0s\ncontexts:\n- context:\ncluster: k0s\nuser: admin\nname: k0s\ncurrent-context: k0s\nkind: Config\npreferences: {}\nusers:\n- name: admin\nuser:\nclient-certificate-data: &lt;redacted&gt;\nclient-key-data: &lt;redacted&gt;\n</code></pre> <p>Once your control plane is ready you can start adding worker nodes into the newly created control plane.</p>"},{"location":"configuration/","title":"Configuration","text":""},{"location":"configuration/#configuration-file-reference","title":"Configuration file reference","text":"<p>The default k0smotron configuration file is a YAML file that contains the following values:</p> <pre><code>apiVersion: k0smotron.io/v1beta1\nkind: Cluster\nmetadata:\nname: k0smotron-test\nspec:\nreplicas: 1\nk0sImage: k0sproject/k0s\nk0sVersion: v1.27.1-k0s.0\nservice:\ntype: NodePort\napiPort: 30443\nkonnectivityPort: 30132\npersistence:\ntype: emptyDir\n</code></pre>"},{"location":"configuration/#spec-key-detail","title":"<code>spec</code> Key Detail","text":"Element Description <code>replicas</code> Replicas is the desired number of replicas of the k0s control planes. If unspecified, defaults to 1. If the value is above 1, k0smotron requires kine datasource URL to be set. <code>k0sImage</code> The k0s image to be deployed. <code>K0sVersion</code> The k0s version to be deployed. <code>externalAddress</code> ExternalAddress defines k0s external address. See https://docs.k0sproject.io/stable/configuration/#specapi <code>service</code> <code>Service</code> defines the service configuration. <code>persistence</code> <code>Persistence</code> defines the persistence configuration. <code>kineDataSourceURL</code> Defines the kine datasource URL. Required for HA controlplane setup. Must be set if replicas &gt; 1. <code>k0sConfig</code> Defines k0sConfig defines the k0s configuration. Note, that some fields will be overwritten by k0smotron. If empty, will be used default configuration. More info: https://docs.k0sproject.io/stable/configuration/"},{"location":"configuration/#specservice","title":"<code>spec.service</code>","text":"Element Description <code>type</code> Service type. Possible values: <code>NodePort</code>,<code>LoadBalancer</code> <code>apiPort</code> Defines the kubernetes API port. <code>konnectivityPort</code> Defines the konnectivity port."},{"location":"configuration/#specpersistence","title":"<code>spec.persistence</code>","text":"Element Description <code>type</code> Persistence type. Possible values: <code>emptyDir</code>,<code>hostPath</code>,<code>pvc</code> <code>hostPath</code> Defines the host path configuration. Will be used as is in case of <code>.spec.persistence.type</code> is <code>hostPath</code>. <code>persistentVolumeClaim</code> Defines the PVC configuration. Will be used as is in case of <code>.spec.persistence.type</code> is <code>pvc</code>."},{"location":"configuration/#k0s-configuration","title":"K0s configuration","text":"<p>K0smotron allows you to configure k0s via <code>spec.k0sConfig</code> field. If empty, will be used default configuration.  More info about the k0s config: https://docs.k0sproject.io/stable/configuration/ </p> <p>Note: some fields will be overwritten by k0smotron. K0smotron will set the following fields:</p> <ul> <li><code>spec.k0sConfig.spec.api.externalAddress</code> will be set to the value of <code>spec.externalAddress</code> if <code>spec.externalAddress</code> is set.     If not, k0smotron will use load balancer IP or try to detect <code>externalAddress</code> out of nodes IP addresses. </li> <li><code>spec.k0sConfig.spec.api.port</code> will be set to the value of <code>spec.service.apiPort</code>.</li> <li><code>spec.k0sConfig.spec.konnectivity.port</code> will be set to the value of <code>spec.service.konnectivityPort</code>.</li> <li><code>spec.k0sConfig.spec.storage.kine.dataSource</code> will be set to the value of <code>spec.kineDataSourceURL</code> if <code>spec.kineDataSourceURL</code> is set.    <code>spec.k0sConfig.spec.storage.type</code> will be set to <code>kine</code>.</li> </ul>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#how-is-k0smotron-different-from-typical-multi-cluster-management-solutions-such-as-tanzu-rancher-etc","title":"How is k0smotron different from typical multi-cluster management solutions such as Tanzu, Rancher etc.?","text":"<p>Most of the existing multi-cluster management solutions provision specific infrastructure for the control planes, in most cases VMs. In all of the cases we've looked at the worker plane infrastructure is also provisioned in the same infrastructure with the control plane and thus not allowing you to fully utilize the capabilities of the management cluster.</p>"},{"location":"faq/#how-is-this-different-for-managed-kubernetes-providers","title":"How is this different for managed Kubernetes providers?","text":"<ul> <li>Control and Flexibility: k0smotron gives you full control over your cluster configurations within your existing Kubernetes cluster, offering unparalleled flexibility.</li> <li>Bring Your Own Workers: Unlike managed Kubernetes providers, k0smotron allows you to connect worker nodes from any infrastructure, providing greater freedom and compatibility.</li> <li>Cost Efficiency: By leveraging your existing Kubernetes cluster, k0smotron helps reduce costs associated with managing separate clusters or paying for additional resources.</li> <li>Homogeneous Setup: k0smotron ensures a consistent configuration across clusters, simplifying maintenance and management tasks.</li> </ul>"},{"location":"faq/#what-is-the-relation-of-k0smotron-with-cluster-api","title":"What is the relation of k0smotron with Cluster API?","text":"<p>While k0smotron currently is a \"standalone\" controller for k0s control planes we're looking to expand this as a full Cluster API provider. Or rather set pf providers as were looking to implement both ControlPlane and Bootstrap providers.</p>"},{"location":"ha/","title":"Highly available controlplanes","text":"<p>As the nature of Kubernetes workloads, in this case the cluster control planes, is quite dynamic it poses a challenge to setup highly available Etcd cluster for the control plane. In k0smotron we're solving the challenge by \"externalizing\" the control plane data storage HA setup.</p> <p>The control planes managed by <code>k0smotron</code> are k0s control planes. As k0s comes with support for using SQL DBs as data store (via Kine) you can use HA databases instead of Etcd. This enables you to use e.g. Postgres operator, MySQL operator or cloud provider managed databases as the data store for the control planes.</p>"},{"location":"ha/#using-postgres-operator","title":"Using Postgres operator","text":"<p>In this example we show how to use Postgres operator to manage the control plane data store.</p> <p>Install the operator following the quicstart guide.</p> <p>Create the database with a custom resource: <pre><code>apiVersion: \"acid.zalan.do/v1\"\nkind: postgresql\nmetadata:\n  name: acid-minimal-cluster\nspec:\n  teamId: \"acid\"\n  volume:\n    size: 10Gi\n  numberOfInstances: 2\n  users:\n    # database owner\n    k0smotron:\n    - superuser\n    - createdb\n\n  databases:\n    kine: k0smotron\n  postgresql:\n    version: \"15\"\n</code></pre></p> <p>Once the database has been setup properly, you can instruct k0smotron to create a control plane using it:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: k0smotron.io/v1beta1\nkind: Cluster\nmetadata:\n  name: k0smotron-test\nspec:\n  replicas: 3\n  service:\n    type: LoadBalancer\n  kineDataSourceURL: postgres://k0smotron:&lt;passwd&gt;@acid-minimal-cluster.default:5432/kine?sslmode=disable\nEOF\n</code></pre> <p>Another option is to use the reference to the secret containing the database credentials:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Secret\nmetadata:\n  name: database-credentials\n  namespace: k0smotron-test\ntype: Opaque\ndata:\n  K0SMOTRON_KINE_DATASOURCE_URL: &lt;base64-encoded-datasource&gt;\n---\napiVersion: k0smotron.io/v1beta1\nkind: Cluster\nmetadata:\n  name: k0smotron-test\nspec:\n  replicas: 3\n  service:\n    type: LoadBalancer\n  kineDataSourceSecretName: database-credentials\nEOF\n</code></pre> <p>Note: The secret must be in the same namespace as the cluster and the key must be <code>K0SMOTRON_KINE_DATASOURCE_URL</code>.</p>"},{"location":"install/","title":"Installation","text":"<p>To install k0smotron, run the following command:</p> <pre><code>kubectl apply -f https://docs.k0smotron.io/v0.4.0/install.yaml\n</code></pre> <p>This install the k0smotron controller manager, all the related CRD definitions and needed RBAC rules.</p> <p>Once the installation is completed you are ready to create your first control planes.</p>"},{"location":"join-nodes/","title":"Join worker nodes","text":"<p>Joining worker nodes is pretty much the exact same process as with k0s in general. You need a join token that enables mutual trust between the worker and controller(s) and which allows the node to join the cluster as worker.</p>"},{"location":"join-nodes/#join-tokens","title":"Join Tokens","text":"<p>To get a token, create a <code>JoinTokenRequest</code> resource:</p> <pre><code>apiVersion: k0smotron.io/v1beta1\nkind: JoinTokenRequest\nmetadata:\nname: my-token\nnamespace: default\nspec:\nclusterRef:\nname: my-cluster\nnamespace: default\n</code></pre> <p>The <code>JoinTokenRequest</code> resource will be processed by the controller and a <code>Secret</code> will be created:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\nname: my-token\nnamespace: default\nlabels:\nk0smotron.io/cluster: my-cluster.default\nk0smotron.io/role: worker\nk0smotron.io/token-request: my-token\ntype: Opaque\ndata:\ntoken: &lt;base64-encoded-token&gt;\n</code></pre> <p>The <code>token</code> field contains the base64-encoded token that can be used to join a worker node to the cluster.</p> <p>To get the decoded token you can use:</p> <pre><code>kubectl get secret my-token -o jsonpath='{.data.token}' | base64 -d\n</code></pre>"},{"location":"join-nodes/#join-nodes","title":"Join nodes","text":"<p>First you need to get the <code>k0s</code> binary on the node:</p> <pre><code>curl -sSLf https://get.k0s.sh | sudo sh\n</code></pre> <p>The download script accepts the following environment variables:</p> Variable Purpose <code>K0S_VERSION=v{{ no such element: dict object['k8s_version'] }}+k0s.0</code> Select the version of k0s to be installed <code>DEBUG=true</code> Output commands and their arguments at execution. <p>Note: Match the k0s version to the version of the control plane you've created.</p> <p>To join the worker, run k0s in the worker mode with the join token you created:</p> <pre><code>sudo k0s install worker --token-file /path/to/token/file\n</code></pre> <pre><code>sudo k0s start\n</code></pre>"},{"location":"join-nodes/#invalidating-tokens","title":"Invalidating tokens","text":"<p>You can limit the validity period by setting the <code>expiry</code> field in the <code>JoinTokenRequest</code> resource:</p> <pre><code>apiVersion: k0smotron.io/v1beta1\nkind: JoinTokenRequest\nmetadata:\nname: my-token\nnamespace: default\nspec:\nclusterRef:\nname: my-cluster\nnamespace: default\nexpiry: 1h\n</code></pre> <p>To invalidate an issued token, delete the <code>JoinTokenRequest</code> resource:</p> <pre><code>kubectl delete jointokenrequest my-token\n</code></pre>"},{"location":"status/","title":"Project status","text":"<p>We\u2019re really early on with the project and as always with any young project there are probably some sharp corners. But with the help of the open source community we plan to iron out those in the coming months.</p> <p>At this point we can not give much guarantees over backwards and forwards compatibility so expect things to break a bit with upcoming releases of k0smotron.</p> <p>We wanted to release k0smotron as early as possible to see whether there is in general interest of running k0s control planes as Kubernetes resources. We are excited about the potential that k0smotron brings to the table, and we look forward to seeing how it can transform the way you manage your Kubernetes deployments.</p>"},{"location":"status/#cluster-api","title":"Cluster API","text":"<p>One of the directions we're looking at is the have k0smotron working as a Cluster API provider for both <code>ControlPlane</code> and worker <code>Bootstrap</code> providers. What this means is that you could utilize Cluster API to provision both the control plane, within the management cluster, and worker nodes in your favourite infrastructure supporting cluster API.</p>"},{"location":"status/#known-limitations","title":"Known limitations","text":"<p>Some of the areas we know have lot of shortcomings currently: - Control Plane configurability: As you know, k0s itself has lot of configurability, we plan to enable full configurability of the k0s control plane - Control plane exposing: Currently k0smotron only supports <code>NodePort</code> and <code>LoadBalancer</code> type services. While that is in itself quite ok quite often there's need to further configure e.g. annotations etc. on the created service to make them play nice with cloud provider implementations. - Updates: While k0smotron is able to update the cluster controlplane easily the update does not strecth into worker nodes. - </p>"},{"location":"usage-overview/","title":"k0smotrong usage","text":"<p>k0smotron can be use either as a \"standalone\" manager for Kubernetes control planes or as a Cluster API controlplane and bootstrap provider.</p>"},{"location":"usage-overview/#standalone","title":"Standalone","text":"<p>In standalone mode k0smotron will manage ONLY the controlplanes running in the management cluster. To get started creating and managing control planes with k0smotron see cluster creation docs.</p>"}]}